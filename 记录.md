0711
构建RL环境（非分层，【分层仍需工作】），其中yardenv需完善【reward机制、再次检查动作机制是否合理】【完善特征提取与使用】
yardGym将环境封装成Gym类，方便直接调用stable_baselines3的PPO【便于后期做消融实验】
【yardGym有问题】
0723

0724
Encoder部分设计：
    ![alt text](image.png)
0726
设计：
1.encoder部分：对同批次输入，进行multi-head self-attention 编码（后续可以尝试用GAT替代，参数更少），获取全局信息
2.decoder部分：将该批次作为query，已存在箱位作为key/value，辅助获取context（上下文信息）
得到编码，再输入RL
【尝试跑通现在代码（使用原RRO环境）】
0729
在ppo中加入维护历史状态的缓存机制
【存在问题：现在RL训练的reward为0，debug中】
0804
可以在原RRO环境中运行。且效果明显优于